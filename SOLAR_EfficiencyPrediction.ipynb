{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Optimized Solar Efficiency Prediction System\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OrdinalEncoder, RobustScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Install and import advanced libraries\n",
        "try:\n",
        "    from lightgbm import LGBMRegressor\n",
        "    HAS_LIGHTGBM = True\n",
        "    print(\"âœ“ LightGBM available\")\n",
        "except ImportError:\n",
        "    print(\"Installing LightGBM...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"lightgbm\"])\n",
        "    from lightgbm import LGBMRegressor\n",
        "    HAS_LIGHTGBM = True\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "    HAS_XGBOOST = True\n",
        "    print(\"âœ“ XGBoost available\")\n",
        "except ImportError:\n",
        "    print(\"Installing XGBoost...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
        "    from xgboost import XGBRegressor\n",
        "    HAS_XGBOOST = True\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostRegressor\n",
        "    HAS_CATBOOST = True\n",
        "    print(\"âœ“ CatBoost available\")\n",
        "except ImportError:\n",
        "    print(\"Installing CatBoost...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"catboost\"])\n",
        "    from catboost import CatBoostRegressor\n",
        "    HAS_CATBOOST = True\n",
        "\n",
        "print(\"\\n=== LOADING DATA ===\")\n",
        "\n",
        "# Load data with correct file names\n",
        "def load_data():\n",
        "    \"\"\"Load train and test data with multiple path attempts\"\"\"\n",
        "    paths_to_try = [\n",
        "        ('train_cleaned.csv', 'test_cleaned.csv'),\n",
        "        ('./train_cleaned.csv', './test_cleaned.csv'),\n",
        "        ('/content/train_cleaned.csv', '/content/test_cleaned.csv'),\n",
        "        ('/content/drive/MyDrive/train_cleaned.csv', '/content/drive/MyDrive/test_cleaned.csv')\n",
        "    ]\n",
        "\n",
        "    for train_path, test_path in paths_to_try:\n",
        "        try:\n",
        "            train_data = pd.read_csv(train_path)\n",
        "            test_data = pd.read_csv(test_path)\n",
        "            print(f\"âœ“ Data loaded from: {train_path}\")\n",
        "            print(f\"  Train shape: {train_data.shape}\")\n",
        "            print(f\"  Test shape: {test_data.shape}\")\n",
        "            return train_data, test_data\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "    raise FileNotFoundError(\"Could not find train_cleaned.csv and test_cleaned.csv files\")\n",
        "\n",
        "train, test = load_data()\n",
        "\n",
        "# Store test IDs\n",
        "test_ids = test['id'].copy()\n",
        "\n",
        "print(f\"Training columns: {list(train.columns)}\")\n",
        "print(f\"Test columns: {list(test.columns)}\")\n",
        "\n",
        "# Check target variable\n",
        "if 'efficiency' in train.columns:\n",
        "    print(f\"Target variable statistics:\")\n",
        "    print(train['efficiency'].describe())\n",
        "else:\n",
        "    print(\"Warning: 'efficiency' column not found!\")\n",
        "\n",
        "print(\"\\n=== DATA PREPROCESSING ===\")\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"Clean and preprocess data\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_cols = ['humidity', 'wind_speed', 'pressure', 'voltage', 'current',\n",
        "                   'module_temperature', 'temperature']\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            # Fill missing values with median\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "            # Remove extreme outliers\n",
        "            Q1 = df[col].quantile(0.05)\n",
        "            Q3 = df[col].quantile(0.95)\n",
        "            df[col] = np.clip(df[col], Q1, Q3)\n",
        "\n",
        "    # Handle categorical columns\n",
        "    categorical_cols = ['string_id', 'error_code', 'installation_type']\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df[col].fillna('unknown', inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Clean data\n",
        "train = clean_data(train)\n",
        "test = clean_data(test)\n",
        "\n",
        "# Encode categorical variables - using already encoded data\n",
        "# The data appears to be pre-encoded with binary columns for categorical features\n",
        "print(\"âœ“ Using pre-encoded categorical features\")\n",
        "\n",
        "print(\"\\n=== FEATURE ENGINEERING ===\")\n",
        "\n",
        "def create_features(df):\n",
        "    \"\"\"Create optimized feature set\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Core electrical features\n",
        "    df['power'] = df['voltage'] * df['current']\n",
        "    df['resistance'] = df['voltage'] / (df['current'] + 1e-8)\n",
        "\n",
        "    # Temperature features\n",
        "    df['temp_diff'] = df['module_temperature'] - df['temperature']\n",
        "    df['temp_ratio'] = df['module_temperature'] / (df['temperature'] + 273.15)\n",
        "\n",
        "    # Solar efficiency features\n",
        "    df['irradiance_temp'] = df['irradiance'] * df['temperature']\n",
        "    df['irradiance_power'] = df['irradiance'] * df['power']\n",
        "    df['soiling_effect'] = df['soiling_ratio'] * df['irradiance']\n",
        "\n",
        "    # Environmental interactions\n",
        "    df['humidity_temp'] = df['humidity'] * df['temperature']\n",
        "    df['pressure_temp'] = df['pressure'] * df['temperature']\n",
        "    df['wind_temp'] = df['wind_speed'] * df['temperature']\n",
        "    df['cloud_irradiance'] = df['cloud_coverage'] * df['irradiance']\n",
        "\n",
        "    # Panel condition features\n",
        "    df['age_maintenance'] = df['panel_age'] * df['maintenance_count']\n",
        "    df['age_soiling'] = df['panel_age'] * df['soiling_ratio']\n",
        "\n",
        "    # Power efficiency indicators\n",
        "    df['power_per_temp'] = df['power'] / (df['temperature'] + 273.15)\n",
        "    df['power_per_irradiance'] = df['power'] / (df['irradiance'] + 1e-8)\n",
        "    df['voltage_efficiency'] = df['voltage'] / (df['module_temperature'] + 273.15)\n",
        "\n",
        "    # Polynomial features for key variables\n",
        "    df['voltage_squared'] = df['voltage'] ** 2\n",
        "    df['current_squared'] = df['current'] ** 2\n",
        "    df['power_squared'] = df['power'] ** 2\n",
        "    df['irradiance_squared'] = df['irradiance'] ** 2\n",
        "\n",
        "    # Log transformations\n",
        "    df['power_log'] = np.log1p(df['power'])\n",
        "    df['voltage_log'] = np.log1p(df['voltage'])\n",
        "    df['irradiance_log'] = np.log1p(df['irradiance'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "train = create_features(train)\n",
        "test = create_features(test)\n",
        "\n",
        "# Prepare features\n",
        "X = train.drop(columns=['id', 'efficiency'])\n",
        "y = train['efficiency']\n",
        "X_test = test.drop(columns=['id'] + (['efficiency'] if 'efficiency' in test.columns else []))\n",
        "\n",
        "# Feature scaling\n",
        "scaler = RobustScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "print(f\"Final feature count: {X.shape[1]}\")\n",
        "\n",
        "print(\"\\n=== MODEL TRAINING ===\")\n",
        "\n",
        "# Optimized model configurations\n",
        "models = {\n",
        "    'LightGBM': LGBMRegressor(\n",
        "        objective='regression',\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=8,\n",
        "        num_leaves=63,\n",
        "        feature_fraction=0.8,\n",
        "        bagging_fraction=0.8,\n",
        "        bagging_freq=5,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        random_state=42,\n",
        "        verbosity=-1\n",
        "    ),\n",
        "\n",
        "    'XGBoost': XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        n_estimators=1000,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=8,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    ),\n",
        "\n",
        "    'CatBoost': CatBoostRegressor(\n",
        "        iterations=1000,\n",
        "        learning_rate=0.05,\n",
        "        depth=8,\n",
        "        l2_leaf_reg=3,\n",
        "        random_seed=42,\n",
        "        verbose=False\n",
        "    ),\n",
        "\n",
        "    'RandomForest': RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "# Cross-validation setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "n_models = len(models)\n",
        "\n",
        "# Initialize prediction arrays\n",
        "oof_preds = np.zeros((len(X), n_models))\n",
        "test_preds = np.zeros((len(X_test), n_models))\n",
        "model_scores = []\n",
        "\n",
        "print(\"Training models with 5-fold CV...\")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled, y)):\n",
        "    print(f\"\\nFold {fold + 1}/5\")\n",
        "\n",
        "    X_train, X_val = X_scaled.iloc[train_idx], X_scaled.iloc[val_idx]\n",
        "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    fold_scores = []\n",
        "\n",
        "    for i, (name, model) in enumerate(models.items()):\n",
        "        # Train model with simplified approach\n",
        "        try:\n",
        "            if name == 'LightGBM':\n",
        "                model.fit(X_train, y_train,\n",
        "                         eval_set=[(X_val, y_val)])\n",
        "            elif name == 'XGBoost':\n",
        "                model.fit(X_train, y_train,\n",
        "                         eval_set=[(X_val, y_val)],\n",
        "                         verbose=False)\n",
        "            elif name == 'CatBoost':\n",
        "                model.fit(X_train, y_train,\n",
        "                         eval_set=(X_val, y_val))\n",
        "            else:\n",
        "                model.fit(X_train, y_train)\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: {name} training issue, using basic fit\")\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "        # Predictions\n",
        "        val_pred = model.predict(X_val)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "        # Store predictions\n",
        "        oof_preds[val_idx, i] = val_pred\n",
        "        test_preds[:, i] += test_pred / kf.n_splits\n",
        "\n",
        "        # Score\n",
        "        rmse = mean_squared_error(y_val, val_pred) ** 0.5\n",
        "        fold_scores.append(rmse)\n",
        "        print(f\"  {name:12s} RMSE: {rmse:.6f}\")\n",
        "\n",
        "    model_scores.append(fold_scores)\n",
        "\n",
        "print(\"\\n=== MODEL PERFORMANCE ===\")\n",
        "avg_scores = np.mean(model_scores, axis=0)\n",
        "model_names = list(models.keys())\n",
        "\n",
        "for i, name in enumerate(model_names):\n",
        "    oof_rmse = mean_squared_error(y, oof_preds[:, i]) ** 0.5\n",
        "    oof_r2 = r2_score(y, oof_preds[:, i])\n",
        "    print(f\"{name:12s} - CV RMSE: {avg_scores[i]:.6f}, OOF RMSE: {oof_rmse:.6f}, RÂ²: {oof_r2:.4f}\")\n",
        "\n",
        "print(\"\\n=== ENSEMBLE PREDICTION ===\")\n",
        "\n",
        "# Create meta-features for stacking\n",
        "meta_train = oof_preds.copy()\n",
        "meta_test = test_preds.copy()\n",
        "\n",
        "# Add ensemble features\n",
        "meta_train_enhanced = np.column_stack([\n",
        "    meta_train,\n",
        "    np.mean(meta_train, axis=1),  # Average\n",
        "    np.std(meta_train, axis=1),   # Standard deviation\n",
        "    np.max(meta_train, axis=1),   # Maximum\n",
        "    np.min(meta_train, axis=1)    # Minimum\n",
        "])\n",
        "\n",
        "meta_test_enhanced = np.column_stack([\n",
        "    meta_test,\n",
        "    np.mean(meta_test, axis=1),\n",
        "    np.std(meta_test, axis=1),\n",
        "    np.max(meta_test, axis=1),\n",
        "    np.min(meta_test, axis=1)\n",
        "])\n",
        "\n",
        "# Scale meta features\n",
        "meta_scaler = RobustScaler()\n",
        "meta_train_scaled = meta_scaler.fit_transform(meta_train_enhanced)\n",
        "meta_test_scaled = meta_scaler.transform(meta_test_enhanced)\n",
        "\n",
        "# Final ensemble model\n",
        "ensemble_model = Ridge(alpha=1.0)\n",
        "ensemble_model.fit(meta_train_scaled, y)\n",
        "\n",
        "# Final predictions\n",
        "final_predictions = ensemble_model.predict(meta_test_scaled)\n",
        "\n",
        "# Ensemble performance\n",
        "ensemble_oof = ensemble_model.predict(meta_train_scaled)\n",
        "ensemble_rmse = mean_squared_error(y, ensemble_oof) ** 0.5\n",
        "ensemble_r2 = r2_score(y, ensemble_oof)\n",
        "\n",
        "print(f\"Ensemble RMSE: {ensemble_rmse:.6f}\")\n",
        "print(f\"Ensemble RÂ²: {ensemble_r2:.4f}\")\n",
        "\n",
        "# Best single model for comparison\n",
        "best_single_idx = np.argmin(avg_scores)\n",
        "best_single_rmse = mean_squared_error(y, oof_preds[:, best_single_idx]) ** 0.5\n",
        "\n",
        "improvement = ((best_single_rmse - ensemble_rmse) / best_single_rmse) * 100\n",
        "print(f\"Improvement over best single model: {improvement:.2f}%\")\n",
        "\n",
        "print(\"\\n=== FINAL SUBMISSION ===\")\n",
        "\n",
        "# Ensure predictions are within reasonable bounds\n",
        "final_predictions = np.clip(final_predictions, 0, 1)\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'efficiency': final_predictions\n",
        "})\n",
        "\n",
        "print(f\"Submission shape: {submission.shape}\")\n",
        "print(f\"Prediction range: [{final_predictions.min():.4f}, {final_predictions.max():.4f}]\")\n",
        "print(f\"Prediction mean: {final_predictions.mean():.4f}\")\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('solar_efficiency_submission.csv', index=False)\n",
        "print(\"âœ“ Submission saved as 'solar_efficiency_submission.csv'\")\n",
        "\n",
        "print(\"\\nSample predictions:\")\n",
        "print(submission.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸŽ¯ OPTIMIZATION COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"âœ“ Final Ensemble RMSE: {ensemble_rmse:.6f}\")\n",
        "print(f\"âœ“ Final Ensemble RÂ²: {ensemble_r2:.4f}\")\n",
        "print(f\"âœ“ Features Used: {X.shape[1]}\")\n",
        "print(f\"âœ“ Models Trained: {len(models)}\")\n",
        "print(f\"âœ“ Predictions Generated: {len(submission)}\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onXJ5CRftY5E",
        "outputId": "5498ba2e-0277-4eea-c951-223135007f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ LightGBM available\n",
            "âœ“ XGBoost available\n",
            "âœ“ CatBoost available\n",
            "\n",
            "=== LOADING DATA ===\n",
            "âœ“ Data loaded from: train_cleaned.csv\n",
            "  Train shape: (20000, 24)\n",
            "  Test shape: (12000, 24)\n",
            "Training columns: ['id', 'temperature', 'irradiance', 'humidity', 'panel_age', 'maintenance_count', 'soiling_ratio', 'voltage', 'current', 'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure', 'efficiency', 'string_id_A1', 'string_id_B2', 'string_id_C3', 'string_id_D4', 'error_code_E00', 'error_code_E01', 'error_code_E02', 'installation_type_dual-axis', 'installation_type_fixed', 'installation_type_tracking']\n",
            "Test columns: ['id', 'temperature', 'irradiance', 'humidity', 'panel_age', 'maintenance_count', 'soiling_ratio', 'voltage', 'current', 'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure', 'efficiency', 'string_id_A1', 'string_id_B2', 'string_id_C3', 'string_id_D4', 'error_code_E00', 'error_code_E01', 'error_code_E02', 'installation_type_dual-axis', 'installation_type_fixed', 'installation_type_tracking']\n",
            "Target variable statistics:\n",
            "count    20000.000000\n",
            "mean         0.510260\n",
            "std          0.140420\n",
            "min          0.000000\n",
            "25%          0.445613\n",
            "50%          0.515709\n",
            "75%          0.590324\n",
            "max          0.987066\n",
            "Name: efficiency, dtype: float64\n",
            "\n",
            "=== DATA PREPROCESSING ===\n",
            "âœ“ Using pre-encoded categorical features\n",
            "\n",
            "=== FEATURE ENGINEERING ===\n",
            "Final feature count: 45\n",
            "\n",
            "=== MODEL TRAINING ===\n",
            "Training models with 5-fold CV...\n",
            "\n",
            "Fold 1/5\n",
            "  LightGBM     RMSE: 0.109482\n",
            "  XGBoost      RMSE: 0.109719\n",
            "  CatBoost     RMSE: 0.105897\n",
            "  RandomForest RMSE: 0.107831\n",
            "\n",
            "Fold 2/5\n",
            "  LightGBM     RMSE: 0.104548\n",
            "  XGBoost      RMSE: 0.103856\n",
            "  CatBoost     RMSE: 0.100610\n",
            "  RandomForest RMSE: 0.102193\n",
            "\n",
            "Fold 3/5\n",
            "  LightGBM     RMSE: 0.102314\n",
            "  XGBoost      RMSE: 0.101430\n",
            "  CatBoost     RMSE: 0.097390\n",
            "  RandomForest RMSE: 0.099755\n",
            "\n",
            "Fold 4/5\n",
            "  LightGBM     RMSE: 0.107918\n",
            "  XGBoost      RMSE: 0.107021\n",
            "  CatBoost     RMSE: 0.103148\n",
            "  RandomForest RMSE: 0.105034\n",
            "\n",
            "Fold 5/5\n",
            "  LightGBM     RMSE: 0.111572\n",
            "  XGBoost      RMSE: 0.111128\n",
            "  CatBoost     RMSE: 0.107103\n",
            "  RandomForest RMSE: 0.108869\n",
            "\n",
            "=== MODEL PERFORMANCE ===\n",
            "LightGBM     - CV RMSE: 0.107167, OOF RMSE: 0.107219, RÂ²: 0.4169\n",
            "XGBoost      - CV RMSE: 0.106631, OOF RMSE: 0.106691, RÂ²: 0.4227\n",
            "CatBoost     - CV RMSE: 0.102830, OOF RMSE: 0.102890, RÂ²: 0.4631\n",
            "RandomForest - CV RMSE: 0.104736, OOF RMSE: 0.104792, RÂ²: 0.4430\n",
            "\n",
            "=== ENSEMBLE PREDICTION ===\n",
            "Ensemble RMSE: 0.102787\n",
            "Ensemble RÂ²: 0.4641\n",
            "Improvement over best single model: 0.10%\n",
            "\n",
            "=== FINAL SUBMISSION ===\n",
            "Submission shape: (12000, 2)\n",
            "Prediction range: [0.2437, 0.8857]\n",
            "Prediction mean: 0.5091\n",
            "âœ“ Submission saved as 'solar_efficiency_submission.csv'\n",
            "\n",
            "Sample predictions:\n",
            "   id  efficiency\n",
            "0   0    0.377193\n",
            "1   1    0.526543\n",
            "2   2    0.520025\n",
            "3   3    0.458062\n",
            "4   4    0.466714\n",
            "\n",
            "==================================================\n",
            "ðŸŽ¯ OPTIMIZATION COMPLETE\n",
            "==================================================\n",
            "âœ“ Final Ensemble RMSE: 0.102787\n",
            "âœ“ Final Ensemble RÂ²: 0.4641\n",
            "âœ“ Features Used: 45\n",
            "âœ“ Models Trained: 4\n",
            "âœ“ Predictions Generated: 12000\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}